{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff6a0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevor/miniconda3/envs/mining/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668cecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/trevor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/trevor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ad9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_string(html_content):\n",
    "    try:\n",
    "        # Kiểm tra nếu đầu vào là NaN hoặc không phải chuỗi\n",
    "        if pd.isna(html_content) or not isinstance(html_content, str):\n",
    "            return \"\"\n",
    "        # Tạo đối tượng BeautifulSoup từ chuỗi HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Trích xuất text, loại bỏ thẻ HTML\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        return f\"Đã có lỗi xảy ra: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9a7b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e12d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.read_csv('data/labeled_data.csv')\n",
    "job_posting = pd.read_csv('data/job_posting.csv', nrows=4306)\n",
    "job_posting['description'] = job_posting['description'].apply(html_to_string)\n",
    "\n",
    "labeled_data['experience_info'] = (\n",
    "    '- Position:\\n' + job_posting['title'].apply(preprocess_text) + '\\n\\n- Requirements: \\n' +\n",
    "    job_posting['description'].apply(preprocess_text)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a18df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_posting_id = job_posting['id']\n",
    "job_posting_id_to_idx = {key: value for key, value in enumerate(job_posting_id)}\n",
    "job_posting_idx_to_id = {value: key for key, value in enumerate(job_posting_id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05e12522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 950571233},\n",
       " {1: 1694753589},\n",
       " {2: 2020176787},\n",
       " {3: 2148428602},\n",
       " {4: 2315295279}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_posting_id_to_idx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf188a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 135/135 [04:15<00:00,  1.90s/it]\n",
      "Batches: 100%|██████████| 135/135 [00:06<00:00, 21.11it/s]\n"
     ]
    }
   ],
   "source": [
    "experience_info_embeddings = model.encode(labeled_data['experience_info'].tolist(), show_progress_bar=True, batch_size=32)\n",
    "experience_location_embeddings = model.encode(job_posting['location'].fillna('').tolist(), show_progress_bar=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f47882e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4306, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_info_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b4e9b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 135/135 [02:15<00:00,  1.00s/it]\n",
      "Batches: 100%|██████████| 135/135 [00:08<00:00, 16.60it/s]\n"
     ]
    }
   ],
   "source": [
    "labeled_data['profile_info'] = (\n",
    "    '- Previous positions:\\n' + labeled_data['headline'].apply(preprocess_text) + '\\n\\n- Experiences:\\n' +\n",
    "    labeled_data['summary'].apply(preprocess_text)\n",
    ")\n",
    "\n",
    "profile_info_embeddings = model.encode(labeled_data['profile_info'].tolist(), show_progress_bar=True, batch_size=32)\n",
    "candidate_location_embeddings = model.encode(labeled_data['location_prof'].fillna('').tolist(), show_progress_bar=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c38ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_id_to_public_id = {key: value for key, value in enumerate(labeled_data['public_id'])}\n",
    "profile_public_id_to_id = {value: key for key, value in enumerate(labeled_data['public_id'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47611c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 'karenpcantor'},\n",
       " {1: 'kathe-j-vera-804566113'},\n",
       " {2: 'andycarfax'},\n",
       " {3: 'carlos-eduardo-valentim'},\n",
       " {4: 'carlos-eduardo-valentim'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_id_to_public_id[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "115a8fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Step 4.5: Normalize Final Similarity Scores to [0, 1]...\n",
      "Min normalized score: 0.0000\n",
      "Max normalized score: 1.0000\n",
      "Step 4.5 completed in 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w1_scores = util.cos_sim(profile_info_embeddings, experience_info_embeddings).numpy()\n",
    "w2_scores = util.cos_sim(candidate_location_embeddings, experience_location_embeddings).numpy()\n",
    "\n",
    "w1_weight = 0.8\n",
    "w2_weight = 0.2\n",
    "final_scores = w1_weight * w1_scores + w2_weight * w2_scores\n",
    "\n",
    "indices = np.arange(len(labeled_data))\n",
    "final_similarity_scores = final_scores[indices, indices]\n",
    "\n",
    "labeled_data['final_similarity_score'] = final_similarity_scores\n",
    "\n",
    "# 4.5: Normalize Final Similarity Scores to [0, 1] using Min-Max Scaling\n",
    "print(\"\\nStarting Step 4.5: Normalize Final Similarity Scores to [0, 1]...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute min and max of final_similarity_score\n",
    "s_min = labeled_data['final_similarity_score'].min()\n",
    "s_max = labeled_data['final_similarity_score'].max()\n",
    "\n",
    "# Avoid division by zero\n",
    "if s_max == s_min:\n",
    "    normalized_scores = np.ones(len(labeled_data)) * 0.5  # If all scores are the same, set to 0.5\n",
    "else:\n",
    "    normalized_scores = (labeled_data['final_similarity_score'] - s_min) / (s_max - s_min)\n",
    "\n",
    "labeled_data['normalized_score'] = normalized_scores\n",
    "\n",
    "# Debug: Check the range of normalized scores\n",
    "print(f\"Min normalized score: {labeled_data['normalized_score'].min():.4f}\")\n",
    "print(f\"Max normalized score: {labeled_data['normalized_score'].max():.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Step 4.5 completed in {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8829a57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.68951225)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ca733ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu kết quả vào result.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "top_k = 10\n",
    "recommendations = []\n",
    "output_file = 'result.json'\n",
    "\n",
    "# Duyệt qua từng ứng viên\n",
    "for candidate_idx in range(final_scores.shape[0]):\n",
    "    # Lấy public_id của ứng viên\n",
    "    candidate_id = profile_public_id_to_id.get(candidate_idx, profile_id_to_public_id[candidate_idx])\n",
    "    \n",
    "    # Lấy điểm số của ứng viên\n",
    "    candidate_scores = final_scores[candidate_idx]\n",
    "    \n",
    "    # Lấy chỉ số của top-k công việc\n",
    "    top_k_indices = np.argsort(candidate_scores)[::-1][:top_k]\n",
    "    \n",
    "    # Lấy job_id và similarity_score\n",
    "    top_k_jobs = [\n",
    "        {\n",
    "            'job_id': job_posting_idx_to_id.get(j, job_posting_id_to_idx[j]),\n",
    "            'similarity_score': float(candidate_scores[j])  # Chuyển sang float để lưu JSON\n",
    "        }\n",
    "        for j in top_k_indices\n",
    "    ]\n",
    "    \n",
    "    # Thêm vào danh sách recommendations\n",
    "    recommendations.append({\n",
    "        'public_id': candidate_id,\n",
    "        'recommendations': top_k_jobs\n",
    "    })\n",
    "\n",
    "# Lưu vào file JSON\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(recommendations, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Đã lưu kết quả vào {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de4dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
